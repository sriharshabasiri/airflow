from airflow import DAG
from airflow.providers.ssh.operators.ssh import SSHOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

default_args = {
    'owner': 'your_name',
    'start_date': days_ago(1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'environment_rebuild',
    default_args=default_args,
    description='Environment Rebuild DAG',
    schedule_interval=None,  # You can set an appropriate schedule interval
    catchup=False,
)

# Parameters for source and target servers
source_server = 'your_source_server'
target_server = 'your_target_server'

# Define the SSH connection IDs for source and target servers
source_server_conn_id = 'your_source_server_connection'
target_server_conn_id = 'your_target_server_connection'

# Task 1: Take backup of target setup on remote target server
backup_target_command = [
    'cp -rp /opt/finacle/disapp_be /central/envrebuild/targetbackup/BE',
    'cp -rp /etc/b2k/DFSR1064/conf /central/envrebuild/targetbackup/etcb2kBE'
]
backup_target_task = SSHOperator(
    task_id='backup_target_setup',
    ssh_conn_id=target_server_conn_id,
    command=backup_target_command,
    dag=dag,
)

# Task 2: Copy source setup to staging on remote source server
copy_source_command = [
    'cp -rp /opt/finacle/dfsapp_be /central/envrebuild/BE',
    'cp -rp /etc/b2k/DFSR1064/conf /central/envrebuild/etcb2kBE'
]
copy_source_task = SSHOperator(
    task_id='copy_source_to_staging',
    ssh_conn_id=source_server_conn_id,
    command=copy_source_command,
    dag=dag,
)

# Task 3: Find and delete backup tars on remote staging server
find_and_delete_command = [
    'find /central/envrebuild/BE /central/envrebuild/etcb2kBE -type f \( -name "*.tar" -o -name "*.tar.gz" \) -exec rm {} \;',
]
find_and_delete_task = SSHOperator(
    task_id='find_and_delete_backups',
    ssh_conn_id=source_server_conn_id,
    command=find_and_delete_command,
    dag=dag,
)

# Task 4: Delete log files on remote staging server
delete_logs_command = [
    'find /central/envrebuild/BE /central/envrebuild/etcb2kBE -type f -name "*.log" -exec rm {} \;',
]
delete_logs_task = SSHOperator(
    task_id='delete_logs',
    ssh_conn_id=source_server_conn_id,
    command=delete_logs_command,
    dag=dag,
)

# Task 5: Create tar files on remote staging server
create_tar_command = [
    'tar -cvf /central/envrebuild/BE.tar -C /central/envrebuild/ BE',
    'tar -cvf /central/envrebuild/etcb2kBE.tar -C /central/envrebuild/ etcb2kBE'
]
create_tar_task = SSHOperator(
    task_id='create_tar_files',
    ssh_conn_id=source_server_conn_id,
    command=create_tar_command,
    dag=dag,
)

# Task 6: Copy to remote target server from remote staging server
copy_to_target_command = [
    'scp /central/envrebuild/BE.tar corebank@targetserver:/opt/finacle',
    'scp /central/envrebuild/etcb2kBE.tar corebank@targetserver:/etc/b2k/DFSR1064/conf'
]
copy_to_target_task = SSHOperator(
    task_id='copy_to_target',
    ssh_conn_id=target_server_conn_id,
    command=copy_to_target_command,
    dag=dag,
)

# Task 7: Replace hostnames on remote target server based on the files list
replace_hostnames_command = [
    'sed -i "s#source_uc_hostname#target_uc_hostname#g" /opt/finacle/*.cfg',
    'sed -i "s#d_finacle_dev01#d_finacle_dev_02#g" /etc/b2k/DFSR1064/conf/*.cfg',
]
replace_hostnames_task = SSHOperator(
    task_id='replace_hostnames',
    ssh_conn_id=target_server_conn_id,
    command=replace_hostnames_command,
    dag=dag,
)

# Set task dependencies as needed
backup_target_task >> copy_source_task >> find_and_delete_task >> delete_logs_task >> create_tar_task >> copy_to_target_task >> replace_hostnames_task

if __name__ == "__main__":
    dag.cli()


testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server
testing bashoperator for task1 - stopping server


from airflow import DAG
from airflow.providers.http.operators.http import SimpleHttpOperator
from datetime import datetime
import json

# Define default_args and DAG configuration
default_args = {
    'owner': 'your_name',
    'start_date': datetime(2023, 1, 1),
    'retries': 1,  # Number of retries on task failure
    'retry_delay': timedelta(minutes=5),  # Delay between retries
}

dag = DAG(
    'servicenow_http_call',
    default_args=default_args,
    description='HTTP call to ServiceNow',
    schedule_interval=None,  # Set your desired schedule interval or None
    catchup=False,  # Set to False if you don't want historical DAG runs
)

# Define the ServiceNow URL and request payload
servicenow_url = 'https://your-servicenow-instance-url.com/api/endpoint'
payload = {
    "sysparm_quantity": "1",
    "variables": {
        "CHG_Ticket": "",
        "query": "",
        "infra_app": "OpenJDK_8",
        "host": "vcld027140",
        "env": "preprod",
        "rhel7": ""
    }
}

# Create an HTTP call task
http_task = SimpleHttpOperator(
    task_id='http_call_to_servicenow',
    method='POST',
    http_conn_id='your_http_conn_id',  # Define your HTTP connection ID in Airflow
    endpoint=servicenow_url,
    headers={"Content-Type": "application/json"},
    data=json.dumps(payload),
    xcom_push=True,  # Push the response to XCom for potential use in subsequent tasks
    dag=dag,
)

# Define task dependencies if needed
# Example: http_task >> other_task

if __name__ == "__main__":
    dag.cli()
